{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with the same Spaceship Titanic data, like the previous Lab. The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Lab, you should try different ensemble methods in order to see if can obtain a better model than before. In order to do a fair comparison, you should perform the same feature scaling, engineering applied in previous Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "spaceship.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceship.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceship['Cabin'] = spaceship['Cabin'].str.split('/').str[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceship.drop(columns=['PassengerId', 'Name'], inplace=True)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the numeric columns (feature selection)\n",
    "numeric_columns = ['Age', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "\n",
    "\n",
    "# extract the numerical variables\n",
    "X = spaceship[numeric_columns]\n",
    "var_target = spaceship['Transported']\n",
    "\n",
    "# Split the data into training and testing sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, var_target, test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "# Create a scaler object (feature standardizer)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Adapt the scaler to the training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Transform data in the test set\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an instance of the normalizer\n",
    "\n",
    "normalizer = MinMaxScaler()\n",
    "\n",
    "normalizer.fit(X_train)\n",
    "\n",
    "X_train_norm = normalizer.transform(X_train)\n",
    "\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n",
    "X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection** - now you will try to apply different ensemble methods in order to get a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       1.00      1.00      1.00       661\n",
      "        True       1.00      1.00      1.00       661\n",
      "\n",
      "    accuracy                           1.00      1322\n",
      "   macro avg       1.00      1.00      1.00      1322\n",
      "weighted avg       1.00      1.00      1.00      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the bagging model(base)\n",
    "bagging_reg = BaggingClassifier(DecisionTreeClassifier(max_depth=20),\n",
    "                               n_estimators=100,\n",
    "                               max_samples = 1000)\n",
    "\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "\n",
    "pred = bagging_reg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.80      0.73      0.77       661\n",
      "        True       0.75      0.82      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the bagging model(normalized)\n",
    "bagging_reg = BaggingClassifier(DecisionTreeClassifier(max_depth=20),\n",
    "                               n_estimators=100,\n",
    "                               max_samples = 1000)\n",
    "\n",
    "bagging_reg.fit(X_train_norm, y_train)\n",
    "\n",
    "pred = bagging_reg.predict(X_test_norm)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.73      0.77       661\n",
      "        True       0.76      0.83      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the bagging model(standardized)\n",
    "bagging_reg = BaggingClassifier(DecisionTreeClassifier(max_depth=20),\n",
    "                               n_estimators=100,\n",
    "                               max_samples = 1000)\n",
    "\n",
    "bagging_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "pred = bagging_reg.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.72      0.77       661\n",
      "        True       0.75      0.84      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pasting model (base)\n",
    "pasting_reg = BaggingClassifier(\n",
    "  estimator=DecisionTreeClassifier(max_depth=20),\n",
    "  n_estimators=100,\n",
    "  max_samples=1000,\n",
    "  bootstrap=False  # This ensures that no bootstrap is done, which is characteristic of Pasting\n",
    ")\n",
    "\n",
    "# Train the model with normalized data\n",
    "pasting_reg.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "pred = pasting_reg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.73      0.77       661\n",
      "        True       0.76      0.84      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.79      0.78      0.78      1322\n",
      "weighted avg       0.79      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pasting model (normalized)\n",
    "pasting_reg = BaggingClassifier(\n",
    "  estimator=DecisionTreeClassifier(max_depth=20),\n",
    "  n_estimators=100,\n",
    "  max_samples=1000,\n",
    "  bootstrap=False  # This ensures that no bootstrap is done, which is characteristic of Pasting\n",
    ")\n",
    "\n",
    "# Train the model with normalized data\n",
    "pasting_reg.fit(X_train_norm, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "pred = pasting_reg.predict(X_test_norm)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.73      0.77       661\n",
      "        True       0.75      0.84      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Pasting model (standardized)\n",
    "pasting_reg = BaggingClassifier(\n",
    "  estimator=DecisionTreeClassifier(max_depth=20),\n",
    "  n_estimators=100,\n",
    "  max_samples=1000,\n",
    "  bootstrap=False  # This ensures that no bootstrap is done, which is characteristic of Pasting\n",
    ")\n",
    "\n",
    "# Train the model with normalized data\n",
    "pasting_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Evaluate the model's performance\n",
    "pred = pasting_reg.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.73      0.77       661\n",
      "        True       0.75      0.83      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize a random forest (base)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=20)\n",
    "# train the model\n",
    "forest.fit(X_train, y_train)\n",
    "#evaluate the model\n",
    "pred = forest.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.73      0.76       661\n",
      "        True       0.75      0.82      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.77      1322\n",
      "weighted avg       0.78      0.78      0.77      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize a random forest(normalized)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=20)\n",
    "# train the model\n",
    "forest.fit(X_train_norm, y_train)\n",
    "#evaluate the model\n",
    "pred = forest.predict(X_test_norm)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.73      0.77       661\n",
      "        True       0.75      0.82      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize a random forest(standardized)\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators=100,\n",
    "                             max_depth=20)\n",
    "# train the model\n",
    "forest.fit(X_train_scaled, y_train)\n",
    "\n",
    "#evaluate the model\n",
    "pred = forest.predict(X_test_scaled)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.66      0.71       661\n",
      "        True       0.71      0.81      0.75       661\n",
      "\n",
      "    accuracy                           0.74      1322\n",
      "   macro avg       0.74      0.74      0.73      1322\n",
      "weighted avg       0.74      0.74      0.73      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize the Gradient Boosting model (base)\n",
    "gb_reg = GradientBoostingClassifier(max_depth=20,\n",
    "                                   n_estimators=100)\n",
    "#train the model\n",
    "gb_reg.fit(X_train, y_train)\n",
    "\n",
    "#evaluate the model\n",
    "pred = gb_reg.predict(X_test)\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.66      0.71       661\n",
      "        True       0.70      0.82      0.76       661\n",
      "\n",
      "    accuracy                           0.74      1322\n",
      "   macro avg       0.74      0.74      0.73      1322\n",
      "weighted avg       0.74      0.74      0.73      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize the Gradient Boosting model (normalized)\n",
    "gb_reg = GradientBoostingClassifier(max_depth=20,\n",
    "                                   n_estimators=100)\n",
    "#train the model\n",
    "gb_reg.fit(X_train_norm, y_train)\n",
    "\n",
    "#evaluate the model\n",
    "pred = gb_reg.predict(X_test_norm)\n",
    "\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.78      0.66      0.71       661\n",
      "        True       0.71      0.81      0.75       661\n",
      "\n",
      "    accuracy                           0.74      1322\n",
      "   macro avg       0.74      0.74      0.73      1322\n",
      "weighted avg       0.74      0.74      0.73      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#initialize the Gradient Boosting model (standardized)\n",
    "gb_reg = GradientBoostingClassifier(max_depth=20,\n",
    "                                   n_estimators=100)\n",
    "#train the model\n",
    "gb_reg.fit(X_train_scaled, y_train)\n",
    "\n",
    "#evaluate the model\n",
    "pred = gb_reg.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anano\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.71      0.75       661\n",
      "        True       0.74      0.83      0.78       661\n",
      "\n",
      "    accuracy                           0.77      1322\n",
      "   macro avg       0.77      0.77      0.77      1322\n",
      "weighted avg       0.77      0.77      0.77      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AdaBoost model (base)\n",
    "ada_reg = AdaBoostClassifier(DecisionTreeClassifier(max_depth=20),\n",
    "                            n_estimators=200)\n",
    "# Training the model\n",
    "ada_reg.fit(X_train, y_train)\n",
    "# Evaluate the model\n",
    "pred = ada_reg.predict(X_test) \n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anano\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.81      0.70      0.75       661\n",
      "        True       0.74      0.84      0.78       661\n",
      "\n",
      "    accuracy                           0.77      1322\n",
      "   macro avg       0.77      0.77      0.77      1322\n",
      "weighted avg       0.77      0.77      0.77      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AdaBoost model (normalized)\n",
    "ada_reg = AdaBoostClassifier(DecisionTreeClassifier(max_depth=20),\n",
    "                            n_estimators=200)\n",
    "# Training the model\n",
    "ada_reg.fit(X_train_norm, y_train)\n",
    "# Evaluate the model\n",
    "pred = ada_reg.predict(X_test_norm) \n",
    "\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anano\\anaconda3\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.82      0.71      0.76       661\n",
      "        True       0.74      0.84      0.79       661\n",
      "\n",
      "    accuracy                           0.78      1322\n",
      "   macro avg       0.78      0.78      0.78      1322\n",
      "weighted avg       0.78      0.78      0.78      1322\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize the AdaBoost model (standardized)\n",
    "ada_reg = AdaBoostClassifier(DecisionTreeClassifier(max_depth=20),\n",
    "                            n_estimators=200)\n",
    "# Training the model\n",
    "ada_reg.fit(X_train_scaled, y_train)\n",
    "# Evaluate the model\n",
    "pred = ada_reg.predict(X_test_scaled) \n",
    "\n",
    "\n",
    "print(classification_report(y_pred = pred, y_true = y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is the best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The best model is random forest because it has the highest accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
