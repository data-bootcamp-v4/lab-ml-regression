{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB | Ensemble Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the data**\n",
    "\n",
    "In this challenge, we will be working with the same Spaceship Titanic data, like the previous Lab. The data can be found here:\n",
    "\n",
    "https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\n",
    "\n",
    "Metadata\n",
    "\n",
    "https://github.com/data-bootcamp-v4/data/blob/main/spaceship_titanic.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this Lab, you should try different ensemble methods in order to see if can obtain a better model than before. In order to do a fair comparison, you should perform the same feature scaling, engineering applied in previous Lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.datasets import  fetch_california_housing\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\n",
    "from sklearn.ensemble import BaggingRegressor, BaggingClassifier, RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor, RandomForestClassifier,GradientBoostingClassifier, AdaBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spaceship = pd.read_csv(\"https://raw.githubusercontent.com/data-bootcamp-v4/data/main/spaceship_titanic.csv\")\n",
    "spaceship.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now perform the same as before:\n",
    "- Feature Scaling\n",
    "- Feature Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Perform Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection** - now you will try to apply different ensemble methods in order to get a better model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bagging and Pasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Adaptive Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which model is the best and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#comment here\n",
    "\n",
    "spaceship.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score  # Make sure to import accuracy_score\n",
    "\n",
    "\n",
    "# Assuming the DataFrame is called spaceship\n",
    "# Load your dataset here (this is just an example)\n",
    "spaceship = pd.DataFrame({\n",
    "    'PassengerId': ['0001_01', '0002_01', '0003_01', '0003_02', '0004_01'],\n",
    "    'HomePlanet': ['Europa', 'Earth', 'Europa', 'Europa', 'Earth'],\n",
    "    'CryoSleep': [False, False, False, False, False],\n",
    "    'Cabin': ['B/0/P', 'F/0/S', 'A/0/S', 'A/0/S', 'F/1/S'],\n",
    "    'Destination': ['TRAPPIST-1e']*5,\n",
    "    'Age': [39.0, 24.0, 58.0, 33.0, 16.0],\n",
    "    'VIP': [False, False, True, False, False],\n",
    "    'RoomService': [0.0, 109.0, 43.0, 0.0, 303.0],\n",
    "    'FoodCourt': [0.0, 9.0, 3576.0, 1283.0, 70.0],\n",
    "    'ShoppingMall': [0.0, 25.0, 0.0, 371.0, 151.0],\n",
    "    'Spa': [0.0, 549.0, 6715.0, 3329.0, 565.0],\n",
    "    'VRDeck': [0.0, 44.0, 49.0, 193.0, 2.0],\n",
    "    'Name': ['Maham Ofracculy', 'Juanna Vines', 'Altark Susent', 'Solam Susent', 'Willy Santantines'],\n",
    "    'Transported': [False, True, False, False, True]  # Target variable\n",
    "})\n",
    "\n",
    "# Step 1: Preprocessing\n",
    "# Encoding categorical variables (CryoSleep, VIP, and Transported)\n",
    "spaceship['CryoSleep'] = spaceship['CryoSleep'].astype(int)\n",
    "spaceship['VIP'] = spaceship['VIP'].astype(int)\n",
    "spaceship['Transported'] = spaceship['Transported'].astype(int)\n",
    "\n",
    "# Drop non-numeric columns\n",
    "X = spaceship.drop(['PassengerId', 'HomePlanet', 'Cabin', 'Destination', 'Name', 'Transported'], axis=1)\n",
    "y = spaceship['Transported']\n",
    "\n",
    "# Step 2: Feature Scaling (Standardization)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Step 3: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: Create models\n",
    "\n",
    "# 4.1 Bagging (Using Decision Trees as base learners)\n",
    "bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, random_state=42)\n",
    "\n",
    "# 4.2 Pasting (Similar to Bagging, but with replacement turned off)\n",
    "pasting_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=100, max_samples=0.8, random_state=42)\n",
    "\n",
    "# 4.3 Random Forest (An ensemble of Decision Trees using bagging)\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4.4 Gradient Boosting (Boosting model)\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# 4.5 AdaBoost (Adaptive Boosting)\n",
    "ada_model = AdaBoostClassifier(estimator=DecisionTreeClassifier(max_depth=1), n_estimators=100, random_state=42)\n",
    "\n",
    "# Step 5: Train models and evaluate them\n",
    "\n",
    "# Fit and evaluate Bagging Model\n",
    "bagging_model.fit(X_train, y_train)\n",
    "y_pred_bagging = bagging_model.predict(X_test)\n",
    "accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n",
    "\n",
    "# Fit and evaluate Pasting Model\n",
    "pasting_model.fit(X_train, y_train)\n",
    "y_pred_pasting = pasting_model.predict(X_test)\n",
    "accuracy_pasting = accuracy_score(y_test, y_pred_pasting)\n",
    "\n",
    "# Fit and evaluate Random Forest Model\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Fit and evaluate Gradient Boosting Model\n",
    "gb_model.fit(X_train, y_train)\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "accuracy_gb = accuracy_score(y_test, y_pred_gb)\n",
    "\n",
    "# Fit and evaluate AdaBoost Model\n",
    "ada_model.fit(X_train, y_train)\n",
    "y_pred_ada = ada_model.predict(X_test)\n",
    "accuracy_ada = accuracy_score(y_test, y_pred_ada)\n",
    "\n",
    "# Step 6: Compare the accuracy of each model\n",
    "print(f\"Accuracy of Bagging: {accuracy_bagging:.4f}\")\n",
    "print(f\"Accuracy of Pasting: {accuracy_pasting:.4f}\")\n",
    "print(f\"Accuracy of Random Forest: {accuracy_rf:.4f}\")\n",
    "print(f\"Accuracy of Gradient Boosting: {accuracy_gb:.4f}\")\n",
    "print(f\"Accuracy of AdaBoost: {accuracy_ada:.4f}\")\n",
    "\n",
    "# Step 7: Determine the best model\n",
    "best_model_name = \"\"\n",
    "best_accuracy = 0\n",
    "\n",
    "if accuracy_bagging > best_accuracy:\n",
    "    best_accuracy = accuracy_bagging\n",
    "    best_model_name = \"Bagging\"\n",
    "    \n",
    "if accuracy_pasting > best_accuracy:\n",
    "    best_accuracy = accuracy_pasting\n",
    "    best_model_name = \"Pasting\"\n",
    "    \n",
    "if accuracy_rf > best_accuracy:\n",
    "    best_accuracy = accuracy_rf\n",
    "    best_model_name = \"Random Forest\"\n",
    "    \n",
    "if accuracy_gb > best_accuracy:\n",
    "    best_accuracy = accuracy_gb\n",
    "    best_model_name = \"Gradient Boosting\"\n",
    "    \n",
    "if accuracy_ada > best_accuracy:\n",
    "    best_accuracy = accuracy_ada\n",
    "    best_model_name = \"AdaBoost\"\n",
    "\n",
    "print(f\"The best model is {best_model_name} with an accuracy of {best_accuracy:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
